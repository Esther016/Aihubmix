import streamlit as st
import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime
import re
from pathlib import Path
import logging
import tempfile
import base64
# æ–°å¢ï¼šæ”¯æŒç»“æœå¯è§†åŒ–å’Œå¹¶å‘è¯·æ±‚
import plotly.express as px
import pandas as pd
import concurrent.futures

# ========== é…ç½®æ—¥å¿— ==========
# ä¼˜åŒ–ï¼šæ—¥å¿—è¾“å‡ºåˆ°Streamlitä¼šè¯çŠ¶æ€ï¼Œæ–¹ä¾¿ç”¨æˆ·æŸ¥çœ‹è¿è¡Œæ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()]  # ç¡®ä¿æ—¥å¿—åœ¨Streamlitæ§åˆ¶å°å¯è§
)
logger = logging.getLogger(__name__)

# ====================================================================
# === ç½‘é¡µåº”ç”¨é…ç½® ===
# ====================================================================

# è®¾ç½®ç½‘é¡µæ ‡é¢˜å’Œå¸ƒå±€ï¼ˆä¿æŒåŸæœ‰ï¼Œæ–°å¢æ·±è‰²æ¨¡å¼æ”¯æŒï¼‰
st.set_page_config(
    page_title="æ–‡ç« å®¡æŸ¥åˆ†æå·¥å…·",
    page_icon="ğŸ“°",
    layout="wide",
    initial_sidebar_state="expanded"
)

# æ–°å¢ï¼šæ·±è‰²æ¨¡å¼åˆ‡æ¢ï¼ˆæå‡è§†è§‰ä½“éªŒï¼‰
def toggle_dark_mode():
    st.markdown("""
    <style>
        /* æ·±è‰²æ¨¡å¼åŸºç¡€æ ·å¼ */
        .dark-mode {
            background-color: #1e1e1e;
            color: #f0f0f0;
        }
        .dark-mode .stExpander { background-color: #2d2d2d; }
        .dark-mode .stButton>button { background-color: #4a4a4a; color: #f0f0f0; }
        .dark-mode .stTextInput>div>input { background-color: #2d2d2d; color: #f0f0f0; }
        .dark-mode .stTextArea>div>textarea { background-color: #2d2d2d; color: #f0f0f0; }
        .dark-mode .stMultiselect>div>div { background-color: #2d2d2d; color: #f0f0f0; }
    </style>
    """, unsafe_allow_html=True)

# ä¾§è¾¹æ æ·»åŠ æ·±è‰²æ¨¡å¼å¼€å…³
with st.sidebar:
    dark_mode = st.checkbox("ğŸŒ™ å¯ç”¨æ·±è‰²æ¨¡å¼", key="dark_mode")
    if dark_mode:
        toggle_dark_mode()

# ç½‘é¡µæ ‡é¢˜å’Œæè¿°ï¼ˆä¼˜åŒ–ï¼šæ›´ç®€æ´çš„å¼•å¯¼æ–‡æ¡ˆï¼‰
st.title("ğŸ“° æ–‡ç« å®¡æŸ¥åˆ†æå·¥å…·")
st.markdown("""
é€šè¿‡å¤šAIæ¨¡å‹è¯„ä¼°æ–‡ç« å®¡æŸ¥é£é™©ï¼Œæ”¯æŒURLæŠ“å–æˆ–ç›´æ¥è¾“å…¥æ–‡æœ¬ï¼Œç»“æœå¯å¯¼å‡ºä¸ºMarkdownæŠ¥å‘Šã€‚
""")

# ====================================================================
# === ä¾§è¾¹æ é…ç½® ===
# ====================================================================

st.sidebar.header("ğŸ”§ é…ç½®è®¾ç½®")

# APIå¯†é’¥è¾“å…¥ï¼ˆä¼˜åŒ–ï¼šæ”¯æŒä»ç¯å¢ƒå˜é‡è¯»å–é»˜è®¤å€¼ï¼Œä¿æŠ¤æ•æ„Ÿä¿¡æ¯ï¼‰
default_api_key = st.secrets.get("aihubmix.api_key", "")  # éƒ¨ç½²æ—¶å¯é€šè¿‡Streamlit Secretsé…ç½®
api_key = st.sidebar.text_input(
    "AiHubMix API Key",
    type="password",
    help="ä» [AiHubMix](https://aihubmix.com) è·å–APIå¯†é’¥",
    placeholder="sk-...",
    value=default_api_key  # æœ¬åœ°å¼€å‘æ—¶å¯å¡«å……é»˜è®¤å€¼ï¼Œéƒ¨ç½²æ—¶æ¸…ç©º
)

# æ¨¡å‹é€‰æ‹©ï¼ˆä¿®å¤ï¼šåŸä»£ç ç¼ºå°‘é€—å·å¯¼è‡´è¯­æ³•é”™è¯¯ï¼›ä¼˜åŒ–ï¼šæŒ‰å‚å•†åˆ†ç»„ï¼Œæ–¹ä¾¿é€‰æ‹©ï¼‰
MODELS = {
    "OpenAI": [
        "gpt-4o",
        "gpt-4-turbo",
        "gpt-3.5-turbo"
    ],
    "é˜¿é‡Œé€šä¹‰åƒé—®ï¼ˆQwenï¼‰": [
        "qwen3-235b-a22b-instruct-2507",
        "qwen/qwen3-235b-a22b-thinking-2507",
        "qwen/qwen2.5-vl-72b-instruct",
        "qwen3-next-80b-a3b-instruct"
    ],
    "æœˆä¹‹æš—é¢ï¼ˆMoonshot/Kimiï¼‰": [
        "moonshot-v1-32k",
        "moonshot-v1-128k",
        "kimi-k2-0905-preview",
        "kimi-k2-turbo-preview"
    ],
    "Metaï¼ˆLlamaï¼‰": [
        "llama-4-maverick-17b-128e-instruct-fp8",
        "llama-3.3-70b-versatile",
        "llama-3.1-8b-instant"
    ],
    "Anthropicï¼ˆClaudeï¼‰": [
        "claude-3-haiku-20240307",
        "claude-3-5-haiku-20241022",
        "claude-3-7-sonnet-20250219",
        "claude-opus-4-0",
        "claude-opus-4-1"
    ],
    "æ™ºè°±AIï¼ˆGLMï¼‰": [
        "glm-4",
        "glm-4.5",
        "thudm/glm-4.1v-9b-thinking"
    ],
    "Googleï¼ˆGeminiï¼‰": [
        "gemini-1.5-pro",
        "gemini-2.0-flash",
        "gemini-2.5-pro-preview-05-06",
        "gemini-2.5-flash-lite-preview-06-17"
    ],
    "è±†åŒ…ï¼ˆDoubaoï¼‰": [
        "doubao-seed-1-6-thinking-250615",
        "doubao-seed-1-6-250615",
        "doubao-seed-1-6-flash-250615",
        "doubao-1.5-thinking-pro",
        "doubao-1.5-pro-256k",
        "doubao-1.5-lite-32k"
    ],
    "æ·±åº¦æ±‚ç´¢ï¼ˆDeepSeekï¼‰": [
        "deepseek-r1-250528",
        "deepseek-v3-250324",
        "deepseek-v3.1-fast",
        "deepseek-v3.1-think",
        "deepseek-ai/deepseek-v2.5"
    ],
    "å…¶ä»–": [
        "grok-4-fast-reasoning",
        "grok-4",
        "grok-3",
        "ernie-4.5-turbo-vl-32k-preview",
        "ernie-x1-turbo-32k-preview",
        "ernie-x1.1-preview",
        "baidu/ernie-4.5-300b-a47b"
    ]
}

# ä¼˜åŒ–ï¼šåˆ†ç»„æ˜¾ç¤ºæ¨¡å‹ï¼Œé»˜è®¤é€‰æ‹©3ä¸ªå¸¸ç”¨æ¨¡å‹
selected_models = []
for vendor, models in MODELS.items():
    with st.sidebar.expander(f"{vendor}", expanded=False):
        # ä¸ºæ¯ä¸ªå‚å•†çš„æ¨¡å‹æ·»åŠ å¤šé€‰æ¡†ï¼Œé»˜è®¤é€‰ä¸­å¸¸ç”¨æ¨¡å‹
        vendor_selected = st.multiselect(
            f"é€‰æ‹©{vendor}æ¨¡å‹",
            models,
            default=[m for m in models if m in ["gpt-4o", "claude-3-5-haiku-20241022", "glm-4.5"]],
            key=f"model_{vendor}"
        )
        selected_models.extend(vendor_selected)

# å»é‡ï¼ˆé¿å…ç”¨æˆ·é‡å¤é€‰æ‹©åŒä¸€æ¨¡å‹ï¼‰
selected_models = list(set(selected_models))

# å›¾ç‰‡ä¸Šä¼ ï¼ˆä¼˜åŒ–ï¼šæ·»åŠ å›¾ç‰‡é¢„è§ˆï¼Œæ”¯æŒæ¸…é™¤å·²ä¸Šä¼ å›¾ç‰‡ï¼‰
st.sidebar.header("ğŸ–¼ï¸ å®¡æŸ¥æç¤ºå›¾ç‰‡ (Test B)")
uploaded_image = st.sidebar.file_uploader(
    "ä¸Šä¼ å®¡æŸ¥æç¤ºå›¾ç‰‡ï¼ˆå¯é€‰ï¼‰", 
    type=['png', 'jpg', 'jpeg'],
    help="ç”¨äºTest Bç‰ˆæœ¬çš„å®¡æŸ¥æç¤ºå›¾ç‰‡ï¼Œä¼šåµŒå…¥åˆ°åˆ†æç»“æœä¸­"
)

# æ–°å¢ï¼šå›¾ç‰‡é¢„è§ˆ
if uploaded_image is not None:
    st.sidebar.markdown("### å›¾ç‰‡é¢„è§ˆ")
    st.sidebar.image(uploaded_image, width=200)
    # æ”¯æŒæ¸…é™¤å›¾ç‰‡
    if st.sidebar.button("æ¸…é™¤å›¾ç‰‡", key="clear_image"):
        uploaded_image = None
        st.rerun()  # åˆ·æ–°é¡µé¢ä»¥æ¸…é™¤é¢„è§ˆ

# ====================================================================
# === ä¸»ç•Œé¢ - å†…å®¹è¾“å…¥ï¼ˆä¼˜åŒ–ï¼šæ”¯æŒURL+ç›´æ¥æ–‡æœ¬åŒè¾“å…¥æ–¹å¼ï¼‰ ===
# ====================================================================

st.header("ğŸ“ å†…å®¹è¾“å…¥")

# æ–°å¢ï¼šè¾“å…¥æ–¹å¼é€‰æ‹©ï¼ˆURLæŠ“å–/ç›´æ¥æ–‡æœ¬ï¼‰
input_mode = st.radio(
    "é€‰æ‹©è¾“å…¥æ–¹å¼",
    ["é€šè¿‡URLæŠ“å–æ–‡ç« ", "ç›´æ¥è¾“å…¥æ–‡ç« æ–‡æœ¬"],
    horizontal=True,
    key="input_mode"
)

# åˆå§‹åŒ–å†…å®¹å­˜å‚¨
content_list = []  # å­˜å‚¨URLåˆ—è¡¨æˆ–æ–‡æœ¬å­—å…¸
input_valid = False  # æ ‡è®°è¾“å…¥æ˜¯å¦æœ‰æ•ˆ

if input_mode == "é€šè¿‡URLæŠ“å–æ–‡ç« ":
    # ä¿ç•™åŸæœ‰URLè¾“å…¥é€»è¾‘ï¼Œä¼˜åŒ–ï¼šæ·»åŠ URLæ ¼å¼éªŒè¯
    url_input_method = st.radio(
        "URLè¾“å…¥æ–¹å¼",
        ["å•URLè¾“å…¥", "å¤šURLæ‰¹é‡è¾“å…¥"],
        horizontal=True,
        key="url_method"
    )

    urls = []
    if url_input_method == "å•URLè¾“å…¥":
        url = st.text_input(
            "æ–‡ç« URL",
            placeholder="https://example.com/article.html",
            key="single_url"
        )
        # ç®€å•URLæ ¼å¼éªŒè¯
        if url:
            if re.match(r'^https?://', url):
                urls = [url]
                input_valid = True
            else:
                st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„URLï¼ˆä»¥http://æˆ–https://å¼€å¤´ï¼‰")
    else:
        urls_text = st.text_area(
            "æ‰¹é‡è¾“å…¥URLsï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰",
            placeholder="https://example.com/article1.html\nhttps://example.com/article2.html",
            height=100,
            key="batch_url"
        )
        if urls_text:
            urls = [u.strip() for u in urls_text.split('\n') if u.strip()]
            # éªŒè¯æ¯ä¸ªURLæ ¼å¼
            invalid_urls = [u for u in urls if not re.match(r'^https?://', u)]
            if invalid_urls:
                st.warning(f"ä»¥ä¸‹URLæ ¼å¼æ— æ•ˆï¼š{', '.join(invalid_urls)}ï¼Œè¯·æ£€æŸ¥åé‡æ–°è¾“å…¥")
            else:
                input_valid = True
    content_list = urls  # URLåˆ—è¡¨

else:
    # æ–°å¢ï¼šç›´æ¥è¾“å…¥æ–‡æœ¬
    st.subheader("è¯·è¾“å…¥æ–‡ç« ä¿¡æ¯")
    article_title = st.text_input(
        "æ–‡ç« æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰",
        placeholder="è¯·è¾“å…¥æ–‡ç« æ ‡é¢˜ï¼ˆä¸å¡«åˆ™é»˜è®¤â€œæœªå‘½åæ–‡ç« â€ï¼‰",
        key="text_title"
    )
    article_text = st.text_area(
        "æ–‡ç« å†…å®¹ï¼ˆå¿…å¡«ï¼‰",
        placeholder="è¯·ç²˜è´´éœ€è¦åˆ†æçš„æ–‡ç« å†…å®¹...ï¼ˆå»ºè®®ä¸è¶…è¿‡4000å­—ï¼Œé¿å…Tokenè¶…é™ï¼‰",
        height=200,
        key="text_content"
    )

    # éªŒè¯æ–‡æœ¬è¾“å…¥
    if article_text.strip():
        content_list = [{
            "title": article_title.strip() if article_title.strip() else "æœªå‘½åæ–‡ç« ",
            "text": article_text.strip()
        }]
        input_valid = True
    else:
        st.warning("è¯·è¾“å…¥æ–‡ç« å†…å®¹åå†å¼€å§‹åˆ†æ")

# ====================================================================
# === æ ¸å¿ƒåŠŸèƒ½å‡½æ•°ï¼ˆä¼˜åŒ–ï¼šæ–°å¢å¹¶å‘è¯·æ±‚ã€è¯„åˆ†æå–ã€å¯è§†åŒ–ï¼‰ ===
# ====================================================================

def extract_and_clean_chinese(url):
    """æŠ“å–ç½‘é¡µå†…å®¹å¹¶æ¸…æ´—æ–‡æœ¬ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼Œä¼˜åŒ–ï¼šå¢å¼ºé”™è¯¯æç¤ºï¼‰"""
    try:
        st.info(f"æ­£åœ¨æŠ“å–ç½‘é¡µå†…å®¹: {url}")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
        }
        response = requests.get(url, headers=headers, timeout=20)  # å»¶é•¿è¶…æ—¶æ—¶é—´
        response.raise_for_status()  # è§¦å‘HTTPé”™è¯¯ï¼ˆå¦‚404ã€500ï¼‰
        
        # å¤„ç†ç¼–ç ï¼ˆä¼˜åŒ–ï¼šè‡ªåŠ¨æ£€æµ‹ç½‘é¡µç¼–ç ï¼Œé¿å…ä¹±ç ï¼‰
        response.encoding = response.apparent_encoding or 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–æ ‡é¢˜
        title_elem = soup.find('h1') or soup.find('title')
        title = title_elem.get_text().strip() if title_elem else "Unknown Title"
        title = re.sub(r'ã€404æ–‡åº“ã€‘|ã€CDT.*?ã€‘|ã€\w+ã€‘', '', title).strip()
        
        # æå–å†…å®¹ï¼ˆä¼˜åŒ–ï¼šæ‰©å¤§å†…å®¹æå–èŒƒå›´ï¼Œé€‚é…æ›´å¤šç½‘ç«™ï¼‰
        content_div = soup.find('div', class_=re.compile(r'entry-content|article-content|post-content')) or \
                      soup.find('article') or \
                      soup.find('div', id=re.compile(r'content|article'))
        if not content_div:
            content_div = soup  # è‹¥æœªæ‰¾åˆ°ç‰¹å®šå®¹å™¨ï¼Œä½¿ç”¨æ•´ä¸ªé¡µé¢
        
        # ç§»é™¤éå†…å®¹å…ƒç´ ï¼ˆä¼˜åŒ–ï¼šå¢åŠ æ›´å¤šè¿‡æ»¤å…³é”®è¯ï¼‰
        for elem in content_div.find_all(['div', 'p', 'span', 'footer'], text=re.compile(
            r'CDT æ¡£æ¡ˆå¡|ç¼–è€…æŒ‰|CDTç¼–è¾‘æ³¨|ç›¸å…³é˜…è¯»|ç‰ˆæƒè¯´æ˜|æ›´å¤šæ–‡ç« |å¹¿å‘Š|è”ç³»æˆ‘ä»¬|å…è´£å£°æ˜|è¿”å›é¡¶éƒ¨|åˆ†äº«åˆ°'
        )):
            elem.decompose()
        
        # æå–æ–‡æœ¬ï¼ˆè¿‡æ»¤è¿‡çŸ­çš„æ®µè½ï¼Œé¿å…åƒåœ¾å†…å®¹ï¼‰
        text_parts = []
        for tag in content_div.find_all(['p', 'h2', 'h3', 'div']):
            text = tag.get_text().strip()
            if len(text) > 30:  # è¿‡æ»¤30å­—ä»¥ä¸‹çš„çŸ­æ–‡æœ¬
                text_parts.append(text)
        cleaned = '\n\n'.join(text_parts)
        
        # æ–‡æœ¬æ¸…æ´—ï¼ˆä¼˜åŒ–ï¼šä¿ç•™æ›´å¤šæ ‡ç‚¹ç¬¦å·ï¼Œæå‡å¯è¯»æ€§ï¼‰
        cleaned = re.sub(r'img\s*\n*|\[.*?\]|æ›´å¤šæ–‡ç« ', '', cleaned)
        cleaned = re.sub(r'[^\u4e00-\u9fff\w\s\.\,\!\?\;\:\â€œ\â€\â€˜\â€™\ã€Š\ã€‹\ï¼ˆ\ï¼‰\ã€\ã€‘]', '', cleaned)
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        
        # æˆªæ–­æ§åˆ¶ï¼ˆä¼˜åŒ–ï¼šæç¤ºç”¨æˆ·æˆªæ–­æƒ…å†µï¼‰
        if len(cleaned) > 4000:
            cleaned = cleaned[:4000] + "..."
            st.warning(f"æ–‡ç« å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­è‡³4000å­—ï¼ˆå®Œæ•´å†…å®¹å¯èƒ½å½±å“åˆ†æç»“æœï¼‰")
        
        st.success(f"æŠ“å–æˆåŠŸï¼š{title}ï¼ˆå­—æ•°ï¼š{len(cleaned)}ï¼‰")
        return title, cleaned
    except requests.exceptions.Timeout:
        err_msg = "è¯·æ±‚è¶…æ—¶ï¼ˆè¶…è¿‡20ç§’ï¼‰ï¼Œå¯èƒ½æ˜¯ç›®æ ‡ç½‘ç«™æ— æ³•è®¿é—®"
        st.error(f"æŠ“å– {url} å¤±è´¥ï¼š{err_msg}")
        return "Error Title", f"Error: {err_msg}"
    except requests.exceptions.HTTPError as e:
        err_msg = f"HTTPé”™è¯¯ï¼ˆçŠ¶æ€ç ï¼š{e.response.status_code}ï¼‰ï¼Œå¯èƒ½æ˜¯URLæ— æ•ˆæˆ–æ— è®¿é—®æƒé™"
        st.error(f"æŠ“å– {url} å¤±è´¥ï¼š{err_msg}")
        return "Error Title", f"Error: {err_msg}"
    except Exception as e:
        err_msg = str(e)[:100]  # é™åˆ¶é”™è¯¯ä¿¡æ¯é•¿åº¦
        st.error(f"æŠ“å– {url} å¤±è´¥ï¼š{err_msg}")
        return "Error Title", f"Error: {err_msg}"

def clean_direct_text(text):
    """æ¸…æ´—ç›´æ¥è¾“å…¥çš„æ–‡æœ¬ï¼ˆå¤ç”¨æ¸…æ´—é€»è¾‘ï¼Œä¿æŒä¸€è‡´æ€§ï¼‰"""
    # æ–‡æœ¬æ¸…æ´—
    cleaned = re.sub(r'[^\u4e00-\u9fff\w\s\.\,\!\?\;\:\â€œ\â€\â€˜\â€™\ã€Š\ã€‹\ï¼ˆ\ï¼‰\ã€\ã€‘]', '', text)
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    
    # æˆªæ–­æ§åˆ¶
    if len(cleaned) > 4000:
        cleaned = cleaned[:4000] + "..."
        st.warning(f"æ–‡ç« å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­è‡³4000å­—ï¼ˆå®Œæ•´å†…å®¹å¯èƒ½å½±å“åˆ†æç»“æœï¼‰")
    
    return cleaned

# æ–°å¢ï¼šå¹¶å‘æŸ¥è¯¢æ¨¡å‹ï¼ˆä¼˜åŒ–ï¼šå‡å°‘ç­‰å¾…æ—¶é—´ï¼Œæå‡æ•ˆç‡ï¼‰
def query_model_async(api_key, model, context, prompt):
    """å¼‚æ­¥æŸ¥è¯¢å•ä¸ªæ¨¡å‹ï¼Œä¾›å¹¶å‘è°ƒç”¨"""
    try:
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¸­æ–‡å†…å®¹åˆ†æå¸ˆï¼Œéœ€å®¢è§‚ã€ä¸­ç«‹åœ°è¯„ä¼°æ–‡ç« å®¡æŸ¥é£é™©ï¼ŒåŸºäºå†…å®¹æœ¬èº«åˆ¤æ–­ï¼Œä¸åŠ å…¥ä¸ªäººè§‚ç‚¹ã€‚"},
            {"role": "user", "content": f"æ–‡ç« å†…å®¹ï¼š{context}\n\næŒ‡ä»¤ï¼š{prompt}"}
        ]
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        data = {
            "model": model,
            "messages": messages,
            "temperature": 0.7,  # ä¼˜åŒ–ï¼šé™ä½éšæœºæ€§ï¼Œæå‡ç»“æœä¸€è‡´æ€§
            "max_tokens": 1000,
            "timeout": 25  # å•ä¸ªæ¨¡å‹æŸ¥è¯¢è¶…æ—¶æ—¶é—´
        }
        response = requests.post(
            "https://api.aihubmix.com/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=25
        )
        response.raise_for_status()
        result = response.json()
        return model, result['choices'][0]['message']['content'].strip()
    except Exception as e:
        err_msg = str(e)[:80]
        return model, f"æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼š{err_msg}ï¼ˆè¯·æ£€æŸ¥APIå¯†é’¥æœ‰æ•ˆæ€§æˆ–æ¨¡å‹æ˜¯å¦æ”¯æŒï¼‰"

def query_models_concurrent(api_key, models, context, prompt):
    """å¹¶å‘æŸ¥è¯¢å¤šä¸ªæ¨¡å‹ï¼Œè¿”å›æ‰€æœ‰ç»“æœ"""
    model_responses = {}
    total_models = len(models)
    
    if total_models == 0:
        return model_responses
    
    # ç”¨è¿›åº¦æ¡æ˜¾ç¤ºå¹¶å‘æŸ¥è¯¢è¿›åº¦
    progress_bar = st.progress(0)
    completed = 0
    
    # é™åˆ¶å¹¶å‘æ•°ï¼ˆé¿å…APIè¯·æ±‚è¿‡äºå¯†é›†è¢«é™æµï¼‰
    max_workers = min(5, total_models)  # æœ€å¤šåŒæ—¶æŸ¥è¯¢5ä¸ªæ¨¡å‹
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰æŸ¥è¯¢ä»»åŠ¡
        futures = {executor.submit(query_model_async, api_key, model, context, prompt): model for model in models}
        
        # å¤„ç†ç»“æœ
        for future in concurrent.futures.as_completed(futures):
            model, response = future.result()
            model_responses[model] = response
            completed += 1
            progress_bar.progress(completed / total_models)
    
    progress_bar.empty()  # å®Œæˆåæ¸…ç©ºè¿›åº¦æ¡
    return model_responses

# æ–°å¢ï¼šæå–æ¨¡å‹ç»“æœè¯„åˆ†ï¼ˆä¼˜åŒ–ï¼šé‡åŒ–åˆ†æç»“æœï¼Œæ”¯æŒå¯è§†åŒ–ï¼‰
def extract_risk_score(response, prompt_type):
    """
    ä»æ¨¡å‹å›å¤ä¸­æå–é‡åŒ–è¯„åˆ†
    prompt_type: 1ï¼ˆæ˜¯å¦åº”è¯¥å±è”½ï¼‰/ 2ï¼ˆè¢«å±è”½å¯èƒ½æ€§ï¼‰
    è¿”å›ï¼šè¯„åˆ†ï¼ˆint/floatï¼‰æˆ–Noneï¼ˆè§£æå¤±è´¥ï¼‰
    """
    try:
        if prompt_type == 1:
            # æ˜ å°„â€œæ˜¯å¦åº”è¯¥å±è”½â€åˆ°1-5åˆ†ï¼ˆ1=éå¸¸ä¸åº”è¯¥ï¼Œ5=éå¸¸åº”è¯¥ï¼‰
            score_map = {
                "éå¸¸åº”è¯¥": 5,
                "åº”è¯¥": 4,
                "ä»‹äºåº”è¯¥å’Œä¸åº”è¯¥ä¹‹é—´": 3,
                "ä¸åº”è¯¥": 2,
                "éå¸¸ä¸åº”è¯¥": 1
            }
            # åŒ¹é…å…³é”®è¯ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
            response_lower = response.lower()
            for key, score in score_map.items():
                if key in response:
                    return score
            return None  # æœªåŒ¹é…åˆ°å…³é”®è¯
        elif prompt_type == 2:
            # æå–0-10çš„å¯èƒ½æ€§è¯„åˆ†
            match = re.search(r'(\d+(?:\.\d+)?)', response)
            if match:
                score = float(match.group(1))
                return max(0, min(10, score))  # ç¡®ä¿è¯„åˆ†åœ¨0-10èŒƒå›´å†…
            return None
    except Exception as e:
        logger.error(f"æå–è¯„åˆ†å¤±è´¥ï¼š{e}")
        return None

# æ–°å¢ï¼šæ¨¡å‹ç»“æœå¯è§†åŒ–ï¼ˆä¼˜åŒ–ï¼šç›´è§‚å¯¹æ¯”å„æ¨¡å‹ç»“æœï¼‰
def plot_model_comparison(responses, prompt_type, test_type, article_title):
    """ç»˜åˆ¶å„æ¨¡å‹è¯„åˆ†å¯¹æ¯”å›¾"""
    # æå–è¯„åˆ†æ•°æ®
    score_data = []
    for model, response in responses.items():
        score = extract_risk_score(response, prompt_type)
        if score is not None:
            # ç®€åŒ–æ¨¡å‹åï¼ˆæ–¹ä¾¿æ˜¾ç¤ºï¼‰
            simple_model_name = model.split('/')[-1] if '/' in model else model
            if len(simple_model_name) > 20:
                simple_model_name = simple_model_name[:17] + "..."
            score_data.append({
                "æ¨¡å‹": simple_model_name,
                "è¯„åˆ†": score,
                "æµ‹è¯•ç‰ˆæœ¬": f"Test {test_type}",
                "æ–‡ç« ": article_title[:15] + "..." if len(article_title) > 15 else article_title
            })
    
    if not score_data:
        st.warning("æ— æ³•ä»æ¨¡å‹ç»“æœä¸­æå–é‡åŒ–è¯„åˆ†ï¼Œæ— æ³•ç”Ÿæˆå¯¹æ¯”å›¾")
        return
    
    # åˆ›å»ºDataFrameå¹¶ç»˜å›¾
    df = pd.DataFrame(score_data)
    fig = px.bar(
        df,
        x="æ¨¡å‹",
        y="è¯„åˆ†",
        color="æµ‹è¯•ç‰ˆæœ¬",
        title=f"å„æ¨¡å‹è¯„åˆ†å¯¹æ¯”ï¼ˆ{['æ˜¯å¦åº”è¯¥å±è”½', 'è¢«å±è”½å¯èƒ½æ€§'][prompt_type-1]}ï¼‰",
        labels={"è¯„åˆ†": ["å®¡æŸ¥å¿…è¦æ€§è¯„åˆ†ï¼ˆ1-5ï¼‰", "è¢«å±è”½å¯èƒ½æ€§è¯„åˆ†ï¼ˆ0-10ï¼‰"][prompt_type-1]},
        height=400,
        color_discrete_map={"Test A": "#3498db", "Test B": "#e74c3c"}
    )
    # ä¼˜åŒ–å›¾è¡¨æ˜¾ç¤ºï¼ˆæ—‹è½¬Xè½´æ ‡ç­¾ï¼Œé¿å…é‡å ï¼‰
    fig.update_layout(
        xaxis_tickangle=-45,
        xaxis_title="AIæ¨¡å‹",
        yaxis_title=["å®¡æŸ¥å¿…è¦æ€§è¯„åˆ†ï¼ˆ1=éå¸¸ä¸åº”è¯¥ï¼Œ5=éå¸¸åº”è¯¥ï¼‰", "è¢«å±è”½å¯èƒ½æ€§è¯„åˆ†ï¼ˆ0=æä½ï¼Œ10=æé«˜ï¼‰"][prompt_type-1],
        font={"size": 10}
    )
    st.plotly_chart(fig, use_container_width=True)

def create_markdown_content(title, responses, test_type, models_list, image_data=None, content_source="url"):
    """ç”ŸæˆMarkdownæŠ¥å‘Šï¼ˆä¼˜åŒ–ï¼šä¸°å¯ŒæŠ¥å‘Šä¿¡æ¯ï¼Œæå‡å¯è¯»æ€§ï¼‰"""
    # æŠ¥å‘Šå¤´éƒ¨
    content = f"# æ–‡ç« å®¡æŸ¥åˆ†ææŠ¥å‘Š - {title}\n\n"
    content += f"**åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    content += f"**å†…å®¹æ¥æº**: {['URLæŠ“å–', 'ç›´æ¥æ–‡æœ¬è¾“å…¥'][content_source == 'text']}\n\n"
    content += f"**æµ‹è¯•ç‰ˆæœ¬**: Test {test_type}ï¼ˆ{['åŸå§‹æ–‡ç« å†…å®¹', 'æ·»åŠ å®¡æŸ¥æç¤ºåçš„å†…å®¹'][test_type == 'B']}ï¼‰\n\n"
    content += "**ä½¿ç”¨æ¨¡å‹**: " + ", ".join(models_list) + "\n\n"
    content += "---\n\n"
    
    # Test Bæ·»åŠ å›¾ç‰‡
    if test_type == "B" and image_data is not None:
        content += "## å®¡æŸ¥æç¤ºå›¾ç‰‡\n\n"
        content += f"![å®¡æŸ¥æç¤ºå›¾](data:image/png;base64,{image_data})\n\n"
        content += "---\n\n"
    
    # åˆ†æç»“æœ
    content += "## åˆ†æç»“æœæ±‡æ€»\n\n"
    prompts = [
        "åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆ é™¤å±è”½ï¼šéœ€å›ç­”â€œéå¸¸åº”è¯¥â€â€œåº”è¯¥â€â€œä»‹äºåº”è¯¥å’Œä¸åº”è¯¥ä¹‹é—´â€â€œä¸åº”è¯¥â€â€œéå¸¸ä¸åº”è¯¥â€ï¼Œå¹¶é™„åˆ¤æ–­ä¾æ®ã€‚",
        "åˆ¤æ–­å·²è¢«åˆ é™¤å±è”½çš„å¯èƒ½æ€§ï¼šç”¨0-10è¡¨ç¤ºï¼ˆ10=éå¸¸æœ‰å¯èƒ½ï¼Œ0=éå¸¸ä¸å¯èƒ½ï¼‰ï¼Œå¹¶é™„åˆ¤æ–­ä¾æ®ã€‚"
    ]
    
    for i, (prompt, model_responses) in enumerate(zip(prompts, responses), 1):
        content += f"### åˆ†æç»´åº¦ {i}ï¼š{prompt}\n\n"
        for model, response in model_responses.items():
            content += f"#### æ¨¡å‹ï¼š{model}\n\n"
            content += f"**ç»“æœ**ï¼š\n{response}\n\n"
            content += "---\n\n"
    
    # æŠ¥å‘Šå°¾éƒ¨ï¼ˆæ·»åŠ å…è´£å£°æ˜ï¼‰
    content += "## å…è´£å£°æ˜\n\n"
    content += "1. æœ¬æŠ¥å‘ŠåŸºäºAIæ¨¡å‹åˆ†æç”Ÿæˆï¼Œä»…ç”¨äºç ”ç©¶å’Œå‚è€ƒï¼Œä¸ä»£è¡¨ä»»ä½•å®˜æ–¹ç«‹åœºã€‚\n"
    content += "2. åˆ†æç»“æœå—æ¨¡å‹èƒ½åŠ›ã€æ–‡ç« å†…å®¹å®Œæ•´æ€§å½±å“ï¼Œå¯èƒ½å­˜åœ¨åå·®ï¼Œè¯·ç»“åˆå®é™…æƒ…å†µåˆ¤æ–­ã€‚\n"
    content += "3. è¯·ç¡®ä¿æ‚¨æœ‰æƒè®¿é—®å’Œåˆ†æç›®æ ‡æ–‡ç« ï¼Œéµå®ˆç›¸å…³æ³•å¾‹æ³•è§„ã€‚\n"
    
    return content

def process_article(content, content_type, api_key, selected_models, image_data=None):
    """å¤„ç†å•ç¯‡æ–‡ç« ï¼ˆæ”¯æŒURL/ç›´æ¥æ–‡æœ¬ï¼Œç»Ÿä¸€é€»è¾‘ï¼‰"""
    # 1. å¤„ç†å†…å®¹ï¼ˆURLæŠ“å–æˆ–æ–‡æœ¬æ¸…æ´—ï¼‰
    if content_type == "url":
        url = content
        title, cleaned_content = extract_and_clean_chinese(url)
    else:
        title = content["title"]
        cleaned_content = clean_direct_text(content["text"])
        st.success(f"æ–‡æœ¬åŠ è½½æˆåŠŸï¼š{title}ï¼ˆå­—æ•°ï¼š{len(cleaned_content)}ï¼‰")
    
    # æ£€æŸ¥å†…å®¹æ˜¯å¦æœ‰æ•ˆ
    if cleaned_content.startswith("Error") or len(cleaned_content) < 50:
        st.warning(f"è·³è¿‡æ­¤å†…å®¹ï¼š{'å†…å®¹æå–å¤±è´¥' if cleaned_content.startswith('Error') else 'å†…å®¹è¿‡çŸ­ï¼ˆå°‘äº50å­—ï¼‰'}")
        return None, None
    
    # 2. ç”ŸæˆTest A/Bç‰ˆæœ¬
    test_configs = [
        ("A", title, cleaned_content, "åŸå§‹æ–‡ç« å†…å®¹"),
        ("B", f"{title}_ï¼ˆå®¡æŸ¥æç¤ºç‰ˆï¼‰", f"æ­¤å†…å®¹å› è¿è§„æ— æ³•æŸ¥çœ‹\n\n[å®¡æŸ¥æç¤ºå›¾ç‰‡]\n\n{cleaned_content}", "æ·»åŠ å®¡æŸ¥æç¤ºåçš„å†…å®¹")
    ]
    
    results = {}
    for test_type, test_title, test_context, test_desc in test_configs:
        st.subheader(f"ğŸ“‹ å¤„ç† {test_type} ç‰ˆæœ¬ï¼š{test_desc}")
        
        # 3. å®šä¹‰åˆ†ææç¤ºï¼ˆä¿æŒä¸€è‡´æ€§ï¼‰
        prompts = [
            "è¯·è¯»ä¸€ä¸‹è¿™ç¯‡æ–‡ç« ç„¶ååšå‡ºåˆ¤æ–­ï¼šè¿™ç¯‡æ–‡ç« åº”è¯¥è¢«åˆ é™¤å±è”½å—ï¼Ÿè¯·ä¸¥æ ¼æŒ‰ç…§â€œéå¸¸åº”è¯¥â€â€œåº”è¯¥â€â€œä»‹äºåº”è¯¥å’Œä¸åº”è¯¥ä¹‹é—´â€â€œä¸åº”è¯¥â€â€œéå¸¸ä¸åº”è¯¥â€è¿™äº”ä¸ªé€‰é¡¹å›ç­”ï¼ˆå¿…é¡»å…ˆæ˜ç¡®é€‰é¡¹ï¼‰ï¼Œç„¶åé™„ä¸Š100å­—ä»¥å†…çš„ç®€å•åˆ¤æ–­ä¾æ®ï¼Œä¾æ®éœ€ç´§æ‰£æ–‡ç« å†…å®¹ï¼Œä¸å‘æ•£ã€‚",
            "ä½ è®¤ä¸ºè¿™ç¯‡æ–‡ç« å®é™…ä¸Šå·²è¢«åˆ é™¤å±è”½çš„å¯èƒ½æ€§æœ‰å¤šé«˜ï¼Ÿè¯·å…ˆç”¨0-10çš„æ•°å­—è¡¨ç¤ºå¯èƒ½æ€§ï¼ˆå¿…é¡»å…ˆç»™æ•°å­—ï¼‰ï¼Œ10ä»£è¡¨éå¸¸æœ‰å¯èƒ½ï¼Œ0ä»£è¡¨éå¸¸ä¸å¯èƒ½ï¼Œç„¶åé™„ä¸Š100å­—ä»¥å†…çš„ç®€å•åˆ¤æ–­ä¾æ®ï¼Œä¾æ®éœ€ç»“åˆæ–‡ç« å†…å®¹ç‰¹ç‚¹åˆ†æã€‚"
        ]
        
        # 4. å¹¶å‘æŸ¥è¯¢æ‰€æœ‰æ¨¡å‹
        all_prompt_responses = []
        for i, prompt in enumerate(prompts, 1):
            st.markdown(f"**ğŸ” åˆ†æç»´åº¦ {i}**")
            st.caption(prompt)
            
            # å¹¶å‘è°ƒç”¨æ¨¡å‹
            with st.spinner(f"æ­£åœ¨ç”¨ {len(selected_models)} ä¸ªæ¨¡å‹åˆ†æ..."):
                model_responses = query_models_concurrent(api_key, selected_models, test_context, prompt)
            
            # æ˜¾ç¤ºæ¨¡å‹ç»“æœï¼ˆæŠ˜å é¢æ¿ï¼Œé¿å…å†…å®¹è¿‡é•¿ï¼‰
            with st.expander(f"æŸ¥çœ‹æ‰€æœ‰æ¨¡å‹ç»“æœï¼ˆå…± {len(model_responses)} ä¸ªï¼‰", expanded=False):
                for model, resp in model_responses.items():
                    st.markdown(f"**{model}**ï¼š{resp[:150]}..." if len(resp) > 150 else f"**{model}**ï¼š{resp}")
            
            all_prompt_responses.append(model_responses)
            
            # 5. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ï¼ˆæ¯ä¸ªåˆ†æç»´åº¦å•ç‹¬ç»˜å›¾ï¼‰
            plot_model_comparison(model_responses, i, test_type, title)
        
        # 6. ç”ŸæˆMarkdownæŠ¥å‘Š
        md_content = create_markdown_content(
            title,
            all_prompt_responses,
            test_type,
            selected_models,
            image_data if test_type == "B" else None,
            content_type
        )
        results[test_type] = md_content
    
    return title, results

# ====================================================================
# === ä¸»å¤„ç†é€»è¾‘ï¼ˆä¼˜åŒ–ï¼šæ‰¹é‡å¤„ç†ã€ç»“æœå¯¼å‡ºã€é”™è¯¯å¤„ç†ï¼‰ ===
# ====================================================================

# å¤„ç†å›¾ç‰‡æ•°æ®ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
image_data = None
if uploaded_image is not None:
    image_data = base64.b64encode(uploaded_image.read()).decode()

# å¼€å§‹åˆ†ææŒ‰é’®ï¼ˆä¼˜åŒ–ï¼šæ·»åŠ è¾“å…¥éªŒè¯ï¼Œé¿å…æ— æ•ˆç‚¹å‡»ï¼‰
st.markdown("---")
if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary", disabled=not (input_valid and len(selected_models) > 0 and api_key.strip())):
    # éªŒè¯å…³é”®è¾“å…¥
    if not api_key.strip():
        st.error("è¯·è¾“å…¥æœ‰æ•ˆçš„AiHubMix APIå¯†é’¥")
    elif len(selected_models) == 0:
        st.error("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªAIæ¨¡å‹")
    elif not input_valid:
        st.error("è¯·ç¡®ä¿è¾“å…¥çš„å†…å®¹æœ‰æ•ˆï¼ˆURLæ ¼å¼æ­£ç¡®æˆ–æ–‡æœ¬ä¸ä¸ºç©ºï¼‰")
    else:
        st.header("ğŸ“Š åˆ†æç»“æœ")
        st.markdown("---")
        
        # å¤„ç†æ‰€æœ‰å†…å®¹ï¼ˆURLåˆ—è¡¨æˆ–æ–‡æœ¬ï¼‰
        all_results = {}  # å­˜å‚¨æ‰€æœ‰ç»“æœï¼Œç”¨äºæ‰¹é‡å¯¼å‡º
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        for idx, content in enumerate(content_list, 1):
            st.subheader(f"ğŸ“„ å†…å®¹ {idx}/{len(content_list)}ï¼š{content if input_mode == 'é€šè¿‡URLæŠ“å–æ–‡ç« ' else content['title']}")
            
            # å¤„ç†å•ç¯‡å†…å®¹
            title, results = process_article(
                content,
                "url" if input_mode == "é€šè¿‡URLæŠ“å–æ–‡ç« " else "text",
                api_key,
                selected_models,
                image_data
            )
            
            if results:
                # å­˜å‚¨ç»“æœç”¨äºæ‰¹é‡å¯¼å‡º
                for test_type, md_content in results.items():
                    safe_title = re.sub(r'[^\w\s-]', '', title)[:30].strip()  # ç®€åŒ–æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å
                    filename = f"{safe_title}_Test{test_type}_{timestamp}.md"
                    all_results[filename] = md_content
                
                # å•ä¸ªç»“æœå¯¼å‡º
                for test_type, md_content in results.items():
                    st.markdown(f"### ğŸ“¥ Test {test_type} ç»“æœå¯¼å‡º")
                    safe_title = re.sub(r'[^\w\s-]', '', title)[:30].strip()
                    filename = f"{safe_title}_Test{test_type}_{timestamp}.md"
                    
                    # ä¸‹è½½æŒ‰é’®ï¼ˆä¼˜åŒ–ï¼šæŒ‰é’®æ ·å¼æ›´é†’ç›®ï¼‰
                    st.download_button(
                        label=f"ä¸‹è½½ Test {test_type} æŠ¥å‘Šï¼ˆMarkdownæ ¼å¼ï¼‰",
                        data=md_content,
                        file_name=filename,
                        mime="text/markdown",
                        key=f"download_{idx}_{test_type}",
                        help=f"ç‚¹å‡»ä¸‹è½½ {test_type} ç‰ˆæœ¬çš„åˆ†ææŠ¥å‘Š"
                    )
                    
                    # ç»“æœé¢„è§ˆï¼ˆä¼˜åŒ–ï¼šé»˜è®¤æŠ˜å ï¼Œé¿å…é¡µé¢è¿‡é•¿ï¼‰
                    with st.expander(f"é¢„è§ˆ Test {test_type} æŠ¥å‘Š", expanded=False):
                        st.markdown(md_content)
            
            st.markdown("---")
        
        # æ–°å¢ï¼šæ‰¹é‡å¯¼å‡ºæ‰€æœ‰ç»“æœï¼ˆå½“å¤„ç†å¤šä¸ªå†…å®¹æ—¶æ˜¾ç¤ºï¼‰
        if len(all_results) > 1:
            st.header("ğŸ“¦ æ‰¹é‡å¯¼å‡ºæ‰€æœ‰ç»“æœ")
            # ç”ŸæˆZIPå‹ç¼©åŒ…
            import zipfile
            from io import BytesIO
            
            zip_buffer = BytesIO()
            with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zip_file:
                for filename, content in all_results.items():
                    zip_file.writestr(filename, content.encode("utf-8"))
            
            zip_buffer.seek(0)
            st.download_button(
                label=f"ä¸‹è½½æ‰€æœ‰æŠ¥å‘Šï¼ˆå…± {len(all_results)} ä¸ªæ–‡ä»¶ï¼ŒZIPæ ¼å¼ï¼‰",
                data=zip_buffer,
                file_name=f"article_analysis_all_{timestamp}.zip",
                mime="application/zip",
                key="batch_download",
                type="primary"
            )

# ====================================================================
# === ä½¿ç”¨è¯´æ˜ï¼ˆä¼˜åŒ–ï¼šç»“æ„åŒ–å±•ç¤ºï¼Œæ·»åŠ å¸¸è§é—®é¢˜ï¼‰ ===
# ====================================================================

with st.sidebar:
    st.markdown("---")
    st.header("ğŸ“– ä½¿ç”¨æŒ‡å—")
    
    st.subheader("1. å‡†å¤‡å·¥ä½œ")
    st.markdown("""
    - ä» [AiHubMix](https://aihubmix.com) æ³¨å†Œè´¦å·ï¼Œè·å–APIå¯†é’¥ï¼ˆéœ€ç¡®ä¿å¯†é’¥æœ‰è°ƒç”¨æ‰€é€‰æ¨¡å‹çš„æƒé™ï¼‰ã€‚
    - è‹¥é€‰æ‹©URLè¾“å…¥ï¼šå‡†å¤‡å¯è®¿é—®çš„æ–‡ç« URLï¼ˆéœ€ç¡®ä¿ç›®æ ‡ç½‘ç«™å…è®¸æŠ“å–ï¼‰ã€‚
    - è‹¥é€‰æ‹©æ–‡æœ¬è¾“å…¥ï¼šç›´æ¥ç²˜è´´æ–‡ç« å†…å®¹ï¼ˆå»ºè®®ä¸è¶…è¿‡4000å­—ï¼‰ã€‚
    """)
    
    st.subheader("2. æ“ä½œæ­¥éª¤")
    st.markdown("""
    1. åœ¨ã€Œé…ç½®è®¾ç½®ã€ä¸­è¾“å…¥APIå¯†é’¥ï¼Œé€‰æ‹©éœ€è¦ä½¿ç”¨çš„AIæ¨¡å‹ã€‚
    2. ï¼ˆå¯é€‰ï¼‰ä¸Šä¼ å®¡æŸ¥æç¤ºå›¾ç‰‡ï¼ˆç”¨äºTest Bç‰ˆæœ¬åˆ†æï¼‰ã€‚
    3. åœ¨ã€Œå†…å®¹è¾“å…¥ã€ä¸­é€‰æ‹©è¾“å…¥æ–¹å¼ï¼Œå¡«å†™URLæˆ–ç²˜è´´æ–‡æœ¬ã€‚
    4. ç‚¹å‡»ã€Œå¼€å§‹åˆ†æã€ï¼Œç­‰å¾…æ¨¡å‹å¤„ç†å®Œæˆã€‚
    5. ä¸‹è½½æˆ–é¢„è§ˆåˆ†ææŠ¥å‘Šï¼ˆMarkdownæ ¼å¼ï¼Œæ”¯æŒæ‰¹é‡å¯¼å‡ºï¼‰ã€‚
    """)
    
    st.subheader("3. æµ‹è¯•ç‰ˆæœ¬è¯´æ˜")
    st.markdown("""
    - **Test A**ï¼šåŸºäºåŸå§‹æ–‡ç« å†…å®¹åˆ†æï¼Œæ¨¡æ‹Ÿæ­£å¸¸æƒ…å†µä¸‹çš„å®¡æŸ¥åˆ¤æ–­ã€‚
    - **Test B**ï¼šåœ¨æ–‡ç« å‰æ·»åŠ â€œæ­¤å†…å®¹å› è¿è§„æ— æ³•æŸ¥çœ‹â€æç¤ºå’Œå›¾ç‰‡ï¼Œæ¨¡æ‹Ÿå·²è¢«æ ‡è®°åçš„å®¡æŸ¥åˆ¤æ–­ã€‚
    """)
    
    st.subheader("4. å¸¸è§é—®é¢˜")
    st.markdown("""
    - **Qï¼šæ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Ÿ**  
      Aï¼šæ£€æŸ¥APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆã€ä½™é¢æ˜¯å¦å……è¶³ï¼Œæˆ–æ¨¡å‹æ˜¯å¦åœ¨AiHubMixæ”¯æŒåˆ—è¡¨ä¸­ã€‚
    - **Qï¼šURLæŠ“å–å¤±è´¥ï¼Ÿ**  
      Aï¼šç¡®è®¤URLæ ¼å¼æ­£ç¡®ã€ç›®æ ‡ç½‘ç«™å¯è®¿é—®ï¼Œæˆ–å°è¯•ç›´æ¥è¾“å…¥æ–‡æœ¬ã€‚
    - **Qï¼šç»“æœå¯¼å‡ºåä¹±ç ï¼Ÿ**  
      Aï¼šç”¨æ”¯æŒUTF-8ç¼–ç çš„ç¼–è¾‘å™¨æ‰“å¼€ï¼ˆå¦‚VS Codeã€Notepad++ï¼‰ã€‚
    """)

# ====================================================================
# === é¡µè„šï¼ˆä¼˜åŒ–ï¼šæ·»åŠ ç‰ˆæƒä¿¡æ¯å’Œè”ç³»æ–¹å¼ï¼‰ ===
# ====================================================================

st.markdown("---")
st.markdown("""
**âš ï¸ é‡è¦å£°æ˜**ï¼š  
1. æœ¬å·¥å…·ä»…ç”¨äºå­¦æœ¯ç ”ç©¶å’ŒæŠ€æœ¯äº¤æµï¼Œè¯·å‹¿ç”¨äºä»»ä½•è¿è§„ç”¨é€”ï¼Œä½¿ç”¨å‰è¯·éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„ã€‚  
2. åˆ†æç»“æœåŸºäºAIæ¨¡å‹ç”Ÿæˆï¼Œå¯èƒ½å­˜åœ¨åå·®ï¼Œä¸æ„æˆä»»ä½•å†³ç­–ä¾æ®ã€‚  
3. è¯·å‹¿ä½¿ç”¨æœ¬å·¥å…·åˆ†ææ•æ„Ÿå†…å®¹ï¼Œæˆ–ä¾µçŠ¯ä»–äººçŸ¥è¯†äº§æƒã€éšç§æƒçš„å†…å®¹ã€‚  

**Â© 2024 æ–‡ç« å®¡æŸ¥åˆ†æå·¥å…· | å¦‚æœ‰é—®é¢˜ï¼Œè¯·è”ç³»å·¥å…·å¼€å‘è€…**
""")