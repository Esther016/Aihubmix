import streamlit as st
import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime
import re
from pathlib import Path
import logging
import tempfile
import base64

# ========== é…ç½®æ—¥å¿— ==========
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# ====================================================================
# === ç½‘é¡µåº”ç”¨é…ç½® ===
# ====================================================================

# è®¾ç½®ç½‘é¡µæ ‡é¢˜å’Œå¸ƒå±€
st.set_page_config(
    page_title="æ–‡ç« å®¡æŸ¥åˆ†æå·¥å…·",
    page_icon="ğŸ“°",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ç½‘é¡µæ ‡é¢˜å’Œæè¿°
st.title("ğŸ“° æ–‡ç« å®¡æŸ¥åˆ†æå·¥å…·")
st.markdown("""
è¿™ä¸ªå·¥å…·å¯ä»¥åˆ†ææ–‡ç« å†…å®¹ï¼Œä½¿ç”¨å¤šä¸ªAIæ¨¡å‹è¯„ä¼°æ–‡ç« çš„å®¡æŸ¥é£é™©ã€‚
è¾“å…¥ä½ çš„APIå¯†é’¥å’Œæ–‡ç« URLsï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æŠ“å–å†…å®¹å¹¶è¿›è¡Œå¤šæ¨¡å‹åˆ†æã€‚
""")

# ====================================================================
# === ä¾§è¾¹æ é…ç½® ===
# ====================================================================

st.sidebar.header("ğŸ”§ é…ç½®è®¾ç½®")

# APIå¯†é’¥è¾“å…¥
api_key = st.sidebar.text_input(
    "AiHubMix API Key",
    type="password",
    help="ä» https://aihubmix.com è·å–ä½ çš„APIå¯†é’¥",
    placeholder="sk-..."
)

# æ¨¡å‹é€‰æ‹©
MODELS = [
    # OpenAIï¼ˆæ³¨æ„ï¼šgpt-5ç³»åˆ—å°šæœªæ­£å¼å‘å¸ƒï¼Œä¿ç•™å¸¸è§å¯ç”¨æ¨¡å‹ï¼‰
    "gpt-4o",
    "gpt-4-turbo",
    "gpt-3.5-turbo"
    
    # Qwenï¼ˆé˜¿é‡Œé€šä¹‰åƒé—®ï¼‰
    "qwen3-235b-a22b-instruct-2507",
    "qwen/qwen3-235b-a22b-thinking-2507",
    "qwen/qwen2.5-vl-72b-instruct",
    "qwen3-next-80b-a3b-instruct",
    
    # Moonshotï¼ˆæœˆä¹‹æš—é¢ï¼‰
    "moonshot-v1-32k",
    "moonshot-v1-128k",
    
    # Llamaï¼ˆMetaï¼‰
    "llama-4-maverick-17b-128e-instruct-fp8",
    "llama-3.3-70b-versatile",
    "llama-3.1-8b-instant",
    
    # Claudeï¼ˆAnthropicï¼‰
    "claude-3-haiku-20240307",
    "claude-3-5-haiku-20241022",
    "claude-3-7-sonnet-20250219",
    "claude-opus-4-0",
    "claude-opus-4-1",
    
    # GLMï¼ˆæ™ºè°±AIï¼‰
    "glm-4",
    "glm-4.5",
    "thudm/glm-4.1v-9b-thinking",
    
    # Geminiï¼ˆGoogleï¼‰
    "gemini-1.5-pro",
    "gemini-2.0-flash",
    "gemini-2.5-pro-preview-05-06",
    "gemini-2.5-flash-lite-preview-06-17",
    
    # Doubaoï¼ˆè±†åŒ…ï¼‰
    "doubao-seed-1-6-thinking-250615",
    "doubao-seed-1-6-250615",
    "doubao-seed-1-6-flash-250615",
    "doubao-1.5-thinking-pro",
    "doubao-1.5-pro-256k",
    "doubao-1.5-lite-32k",
    
    # DeepSeekï¼ˆæ·±åº¦æ±‚ç´¢ï¼‰
    "deepseek-r1-250528",
    "deepseek-v3-250324",
    "deepseek-v3.1-fast",
    "deepseek-v3.1-think",
    "deepseek-ai/deepseek-v2.5",
    
    # Kimiï¼ˆMoonshotï¼‰
    "kimi-k2-0905-preview",
    "kimi-k2-turbo-preview",
    
    # Grokï¼ˆX.AIï¼‰
    "grok-4-fast-reasoning",
    "grok-4",
    "grok-3",
    
    # Ernieï¼ˆç™¾åº¦æ–‡å¿ƒä¸€è¨€ï¼‰
    "ernie-4.5-turbo-vl-32k-preview",
    "ernie-x1-turbo-32k-preview",
    "ernie-x1.1-preview",
    "baidu/ernie-4.5-300b-a47b"
]

selected_models = st.sidebar.multiselect(
    "é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹",
    MODELS,
    default=["gpt-4o", "claude-3-5-haiku-20241022", "glm-4.5"]
)

# å›¾ç‰‡ä¸Šä¼ 
st.sidebar.header("ğŸ–¼ï¸ å®¡æŸ¥æç¤ºå›¾ç‰‡ (Test B)")
uploaded_image = st.sidebar.file_uploader(
    "ä¸Šä¼ å®¡æŸ¥æç¤ºå›¾ç‰‡", 
    type=['png', 'jpg', 'jpeg'],
    help="ç”¨äºTest Bç‰ˆæœ¬çš„å®¡æŸ¥æç¤ºå›¾ç‰‡"
)

# ====================================================================
# === ä¸»ç•Œé¢ - URLè¾“å…¥ ===
# ====================================================================

st.header("ğŸ“ è¾“å…¥æ–‡ç« URLs")

url_input_method = st.radio(
    "URLè¾“å…¥æ–¹å¼",
    ["å•URLè¾“å…¥", "å¤šURLæ‰¹é‡è¾“å…¥"],
    horizontal=True
)

urls = []

if url_input_method == "å•URLè¾“å…¥":
    url = st.text_input(
        "æ–‡ç« URL",
        placeholder="https://example.com/article.html"
    )
    if url:
        urls.append(url)
else:
    urls_text = st.text_area(
        "æ‰¹é‡è¾“å…¥URLs (æ¯è¡Œä¸€ä¸ª)",
        placeholder="https://example.com/article1.html\nhttps://example.com/article2.html",
        height=100
    )
    if urls_text:
        urls = [url.strip() for url in urls_text.split('\n') if url.strip()]

# ====================================================================
# === æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ===
# ====================================================================

def extract_and_clean_chinese(url):
    """æŠ“å–ç½‘é¡µå†…å®¹å¹¶æ¸…æ´—æ–‡æœ¬"""
    try:
        st.info(f"æ­£åœ¨æŠ“å–ç½‘é¡µå†…å®¹: {url}")
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æå–æ ‡é¢˜å¹¶æ¸…ç†
        title_elem = soup.find('h1') or soup.find('title')
        title = title_elem.get_text().strip() if title_elem else "Unknown Title"
        title = re.sub(r'ã€404æ–‡åº“ã€‘|ã€CDT.*?ã€‘|ã€\w+ã€‘', '', title).strip()
        
        # æå–å†…å®¹
        content_div = soup.find('div', class_='entry-content') or soup.find('article')
        if not content_div:
            content_div = soup
        
        # ç§»é™¤éåŸæ–‡ç« éƒ¨åˆ†
        for elem in content_div.find_all(['div', 'p', 'span'], text=re.compile(r'CDT æ¡£æ¡ˆå¡|ç¼–è€…æŒ‰|CDTç¼–è¾‘æ³¨|ç›¸å…³é˜…è¯»|ç‰ˆæƒè¯´æ˜|æ›´å¤šæ–‡ç« ')):
            elem.decompose()
        
        text_parts = [p.get_text().strip() for p in content_div.find_all(['p', 'h2', 'h3']) if len(p.get_text().strip()) > 20]
        cleaned = '\n\n'.join(text_parts)
        
        # é¢å¤–æ¸…æ´—
        cleaned = re.sub(r'img\s*\n*|\[.*?\]|æ›´å¤šæ–‡ç« ', '', cleaned)
        cleaned = re.sub(r'[^\u4e00-\u9fff\w\s\.\,\!\?\(\)\[\]\â€œ\â€\ã€Š\ã€‹]', '', cleaned)
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        
        # æˆªæ–­ä»¥é¿å… Token è¶…é™
        if len(cleaned) > 4000:
            cleaned = cleaned[:4000] + "..."
            
        st.success(f"æŠ“å–æˆåŠŸ: {title}")
        return title, cleaned
    except Exception as e:
        st.error(f"æŠ“å– {url} æ—¶å‡ºé”™: {e}")
        return "Error Title", f"Error: {str(e)}"

def query_model(api_key, model, context, prompt):
    """é€šè¿‡ AiHubMix API å‘æ¨¡å‹å‘é€æŸ¥è¯¢"""
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¸­æ–‡åˆ†æå¸ˆã€‚"},
        {"role": "user", "content": f"æ–‡ç« å†…å®¹ï¼š{context}\n\næŒ‡ä»¤ï¼š{prompt}"}
    ]
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    data = {
        "model": model,
        "messages": messages,
        "temperature": 1.0,
        "max_tokens": 1000
    }
    try:
        with st.spinner(f"æ­£åœ¨æŸ¥è¯¢ {model}..."):
            response = requests.post("https://api.aihubmix.com/v1/chat/completions", headers=headers, json=data, timeout=30)
            response.raise_for_status()
            result = response.json()
            content = result['choices'][0]['message']['content'].strip()
        return content
    except Exception as e:
        return f"Error: {str(e)}"

def create_markdown_content(title, responses, test_type, models_list, image_data=None):
    """ç”ŸæˆMarkdownå†…å®¹"""
    content = f"# {title}\n\n"
    content += "**ä½¿ç”¨çš„æ¨¡å‹:**\n"
    content += ", ".join(models_list) + "\n\n"
    content += f"å¤„ç†æ—¥æœŸ: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n"
    
    # Test Bç‰ˆæœ¬çš„ç‰¹æ®Šå¤„ç†
    if test_type == "B":
        if image_data:
            content += f"![å®¡æŸ¥æç¤ºå›¾](data:image/png;base64,{image_data})\n\n"
        content += f"**æ­¤æ–‡ç« å› è¿è§„å·²ç»æ— æ³•æŸ¥çœ‹ï¼Œä»¥ä¸‹å†…å®¹ä¸ºå±è”½å‰æ‰‹åŠ¨ä¿å­˜**\n\n"
        content += "---\n\n"
    
    content += "## å®¡æŸ¥æ¯”è¾ƒåˆ†æ\n"
    content += "- æç¤ºè¯­ 1: åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆ é™¤å±è”½\n"
    content += "- æç¤ºè¯­ 2: åˆ¤æ–­å·²è¢«åˆ é™¤å±è”½çš„å¯èƒ½æ€§\n\n"
    
    prompts = [
        "è¯·è¯»ä¸€ä¸‹è¿™ç¯‡æ–‡ç« ç„¶ååšå‡ºåˆ¤æ–­ï¼šè¿™ç¯‡æ–‡ç« åº”è¯¥è¢«åˆ é™¤å±è”½å—ï¼Ÿè¯·å›ç­”â€œéå¸¸åº”è¯¥â€ï¼Œâ€œåº”è¯¥â€ï¼Œâ€œä»‹äºåº”è¯¥å’Œä¸åº”è¯¥ä¹‹é—´â€ï¼Œâ€œä¸åº”è¯¥â€ï¼Œâ€œéå¸¸ä¸åº”è¯¥â€ï¼Œå¹¶é™„ä¸Šä¸€æ®µç®€å•çš„åˆ¤æ–­ä¾æ®ã€‚",
        "ä½ è®¤ä¸ºè¿™ç¯‡æ–‡ç« å®é™…ä¸Šå·²è¢«åˆ é™¤å±è”½çš„å¯èƒ½æ€§æœ‰å¤šé«˜ï¼Ÿç”¨0-10è¡¨ç¤ºå¯èƒ½æ€§ï¼Œ10ä»£è¡¨éå¸¸æœ‰å¯èƒ½ï¼Œ0ä»£è¡¨éå¸¸ä¸å¯èƒ½ã€‚"
    ]
    
    for i, (prompt, model_responses) in enumerate(zip(prompts, responses), 1):
        content += f"## æç¤ºè¯­ {i}: {prompt}\n\n"
        for model, response in model_responses.items():
            content += f"### æ¨¡å‹: {model}\n\n{response}\n\n---\n\n"
    
    return content

def process_article(url, api_key, selected_models, image_data=None):
    """å¤„ç†å•ä¸ªæ–‡ç« """
    title, cleaned_text = extract_and_clean_chinese(url)
    
    if cleaned_text.startswith("Error"):
        st.warning(f"è·³è¿‡æ­¤æ–‡ç« ï¼Œå› ä¸ºå†…å®¹æå–å¤±è´¥: {url}")
        return None, None
    
    results = {}
    
    # Test A: çº¯æ–‡å­—ç‰ˆæœ¬
    test_a_title = title
    test_a_context = cleaned_text
    
    # Test B: åŠ censorship treatment
    test_b_title = title + "_æ­¤å†…å®¹å› è¿è§„æ— æ³•æŸ¥çœ‹"
    test_b_context = "æ­¤å†…å®¹å› è¿è§„æ— æ³•æŸ¥çœ‹\n\n[å®¡æŸ¥æç¤ºå›¾ç‰‡]\n\n" + cleaned_text
    
    for test_type, test_title, test_context in [("A", test_a_title, test_a_context), ("B", test_b_title, test_b_context)]:
        st.subheader(f"å¤„ç† {test_type} ç‰ˆæœ¬")
        all_prompt_responses = []
        
        prompts = [
            "è¯·è¯»ä¸€ä¸‹è¿™ç¯‡æ–‡ç« ç„¶ååšå‡ºåˆ¤æ–­ï¼šè¿™ç¯‡æ–‡ç« åº”è¯¥è¢«åˆ é™¤å±è”½å—ï¼Ÿè¯·å›ç­”â€œéå¸¸åº”è¯¥â€ï¼Œâ€œåº”è¯¥â€ï¼Œâ€œä»‹äºåº”è¯¥å’Œä¸åº”è¯¥ä¹‹é—´â€ï¼Œâ€œä¸åº”è¯¥â€ï¼Œâ€œéå¸¸ä¸åº”è¯¥â€ï¼Œå¹¶é™„ä¸Šä¸€æ®µç®€å•çš„åˆ¤æ–­ä¾æ®ã€‚",
            "ä½ è®¤ä¸ºè¿™ç¯‡æ–‡ç« å®é™…ä¸Šå·²è¢«åˆ é™¤å±è”½çš„å¯èƒ½æ€§æœ‰å¤šé«˜ï¼Ÿç”¨0-10è¡¨ç¤ºå¯èƒ½æ€§ï¼Œ10ä»£è¡¨éå¸¸æœ‰å¯èƒ½ï¼Œ0ä»£è¡¨éå¸¸ä¸å¯èƒ½ã€‚"
        ]
        
        for i, prompt in enumerate(prompts, 1):
            st.write(f"**æç¤ºè¯­ {i}:** {prompt}")
            model_responses = {}
            
            progress_bar = st.progress(0)
            for idx, model in enumerate(selected_models):
                response = query_model(api_key, model, test_context, prompt)
                model_responses[model] = response
                progress_bar.progress((idx + 1) / len(selected_models))
            
            all_prompt_responses.append(model_responses)
        
        # ç”ŸæˆMarkdownå†…å®¹
        md_content = create_markdown_content(
            test_title, all_prompt_responses, test_type, selected_models, 
            image_data if test_type == "B" else None
        )
        results[test_type] = md_content
    
    return title, results

# ====================================================================
# === ä¸»å¤„ç†é€»è¾‘ ===
# ====================================================================

# å¤„ç†å›¾ç‰‡æ•°æ®
image_data = None
if uploaded_image is not None:
    image_data = base64.b64encode(uploaded_image.read()).decode()

# å¼€å§‹å¤„ç†æŒ‰é’®
if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary"):
    # éªŒè¯è¾“å…¥
    if not api_key or api_key == "sk-...":
        st.error("è¯·è¾“å…¥æœ‰æ•ˆçš„AiHubMix APIå¯†é’¥")
    elif not urls:
        st.error("è¯·è¾“å…¥è‡³å°‘ä¸€ä¸ªURL")
    elif not selected_models:
        st.error("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªæ¨¡å‹")
    else:
        st.header("ğŸ“Š åˆ†æç»“æœ")
        
        # å¤„ç†æ¯ä¸ªURL
        for url in urls:
            st.markdown(f"---")
            st.subheader(f"åˆ†æ: {url}")
            
            title, results = process_article(url, api_key, selected_models, image_data)
            
            if results:
                # æä¾›ä¸‹è½½é“¾æ¥
                for test_type, md_content in results.items():
                    safe_title = re.sub(r'[^\w\s-]', '', title)[:50].strip()
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"{safe_title}_Test{test_type}_{timestamp}.md"
                    
                    st.download_button(
                        label=f"ğŸ“¥ ä¸‹è½½ Test {test_type} ç»“æœ",
                        data=md_content,
                        file_name=filename,
                        mime="text/markdown",
                        key=f"download_{url}_{test_type}"
                    )
                    
                    # æ˜¾ç¤ºé¢„è§ˆ
                    with st.expander(f"é¢„è§ˆ Test {test_type} ç»“æœ"):
                        st.markdown(md_content)

# ====================================================================
# === ä½¿ç”¨è¯´æ˜ ===
# ====================================================================

with st.sidebar:
    st.header("ğŸ“– ä½¿ç”¨è¯´æ˜")
    st.markdown("""
    1. **è·å–APIå¯†é’¥**: ä» [AiHubMix](https://aihubmix.com) æ³¨å†Œå¹¶è·å–APIå¯†é’¥
    2. **è¾“å…¥URL**: è¾“å…¥è¦åˆ†æçš„æ–‡ç« URL
    3. **é€‰æ‹©æ¨¡å‹**: é€‰æ‹©è¦ä½¿ç”¨çš„AIæ¨¡å‹
    4. **ä¸Šä¼ å›¾ç‰‡** (å¯é€‰): ä¸ºTest Bç‰ˆæœ¬ä¸Šä¼ å®¡æŸ¥æç¤ºå›¾ç‰‡
    5. **å¼€å§‹åˆ†æ**: ç‚¹å‡»å¼€å§‹åˆ†ææŒ‰é’®
    6. **ä¸‹è½½ç»“æœ**: åˆ†æå®Œæˆåä¸‹è½½Markdownæ ¼å¼çš„æŠ¥å‘Š
    
    **æµ‹è¯•ç‰ˆæœ¬è¯´æ˜:**
    - **Test A**: åŸå§‹æ–‡ç« å†…å®¹åˆ†æ
    - **Test B**: æ·»åŠ å®¡æŸ¥æç¤ºåçš„åˆ†æ
    """)

# ====================================================================
# === é¡µè„š ===
# ====================================================================

st.markdown("---")
st.markdown(
    "**æ³¨æ„**: è¿™ä¸ªå·¥å…·ä»…ç”¨äºç ”ç©¶å’Œåˆ†æç›®çš„ã€‚è¯·ç¡®ä¿ä½ æœ‰æƒè®¿é—®å’Œåˆ†æç›®æ ‡ç½‘ç«™çš„å†…å®¹ã€‚"
)